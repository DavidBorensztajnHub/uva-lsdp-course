{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PAN_nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRBaXfd5Zu3y"
      },
      "source": [
        "# Pan nn Text Classification Test\r\n",
        "Trying to classify the age of an author based on a written conversation\r\n",
        "\r\n",
        "## 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHHvR_ghSOpH",
        "outputId": "4886223e-cd49-476f-e738-3f479008f8d7"
      },
      "source": [
        "!pip install torchtext==0.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4 in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.8.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwGxDCFTNGF2",
        "outputId": "6410bd45-5b58-418f-de16-792341b555d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "\n",
        "from torchtext.datasets import text_classification\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import nltk\n",
        "import plotly.express as px\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import plotly.graph_objs as go\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from collections import defaultdict, OrderedDict, Counter\n",
        "import operator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP1rrVDBuGkQ"
      },
      "source": [
        "## 2. Load and preprocces the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrW8KP_mNPkj"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Language, speech and dialogue processing/Datasets/df_pan_train.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll3zoy2sNR1T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "c0e95235-4e2b-4bf9-b584-ba1405719486"
      },
      "source": [
        "df = df.drop(['Unnamed: 0', 'lang', 'id'], axis=1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age_group</th>\n",
              "      <th>gender</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20s</td>\n",
              "      <td>female</td>\n",
              "      <td>The utilization of this item of therapy gear i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20s</td>\n",
              "      <td>female</td>\n",
              "      <td>Before, an individual additional learn about t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30s</td>\n",
              "      <td>male</td>\n",
              "      <td>The vending device organization is one particu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30s</td>\n",
              "      <td>male</td>\n",
              "      <td>National Treasure - three Stars (Excellent)&lt;br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20s</td>\n",
              "      <td>female</td>\n",
              "      <td>Based in Southwest Louisiana, the Law Office o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401988</th>\n",
              "      <td>30s</td>\n",
              "      <td>male</td>\n",
              "      <td>&lt;a href=\"http://en.pan.netcom/go/out/url=-aHR0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401989</th>\n",
              "      <td>20s</td>\n",
              "      <td>male</td>\n",
              "      <td>Microsoft Office 2010 is the newest model (as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401990</th>\n",
              "      <td>20s</td>\n",
              "      <td>male</td>\n",
              "      <td>Prior to beginning the set up process of the a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401991</th>\n",
              "      <td>20s</td>\n",
              "      <td>male</td>\n",
              "      <td>Prior to starting up the installation process ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401992</th>\n",
              "      <td>30s</td>\n",
              "      <td>female</td>\n",
              "      <td>Hello. My name is Angelique'. I am 43yrs young...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>401993 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age_group  gender                                               text\n",
              "0            20s  female  The utilization of this item of therapy gear i...\n",
              "1            20s  female  Before, an individual additional learn about t...\n",
              "2            30s    male  The vending device organization is one particu...\n",
              "3            30s    male  National Treasure - three Stars (Excellent)<br...\n",
              "4            20s  female  Based in Southwest Louisiana, the Law Office o...\n",
              "...          ...     ...                                                ...\n",
              "401988       30s    male  <a href=\"http://en.pan.netcom/go/out/url=-aHR0...\n",
              "401989       20s    male  Microsoft Office 2010 is the newest model (as ...\n",
              "401990       20s    male  Prior to beginning the set up process of the a...\n",
              "401991       20s    male  Prior to starting up the installation process ...\n",
              "401992       30s  female  Hello. My name is Angelique'. I am 43yrs young...\n",
              "\n",
              "[401993 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdDOi2IThlDd"
      },
      "source": [
        "df_sub = df[:10]\n",
        "df_sub.to_csv('/content/drive/MyDrive/Language, speech and dialogue processing/sub_PAN.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE6EoN5DBqTF"
      },
      "source": [
        "ages = df[\"age_group\"].unique().tolist()\n",
        "ages.sort()\n",
        "age_dict = {}\n",
        "for age in ages:\n",
        "  age_dict[age] = df.loc[df['age_group'] == age]\n",
        "\n",
        "for key, value in age_dict.items():\n",
        "  age_dict[key] = value\n",
        "  \n",
        "print(age_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQxrhL2F_eG"
      },
      "source": [
        "# Same range as in pan13 dataset\n",
        "\n",
        "dataframes_10s = []\n",
        "dataframes_20s = []\n",
        "dataframes_30s = []\n",
        "for key, value in age_dict.items():\n",
        "  if key == '10s':\n",
        "    dataframes_10s.append(value)\n",
        "  elif key == '20s':\n",
        "    dataframes_20s.append(value)\n",
        "  elif '30s':\n",
        "    dataframes_30s.append(value)\n",
        "\n",
        "df_10s = pd.concat(dataframes_10s)\n",
        "df_20s = pd.concat(dataframes_20s)\n",
        "df_30s = pd.concat(dataframes_30s)\n",
        "\n",
        "all_dataframes = [df_10s, df_20s, df_30s]\n",
        "\n",
        "min_len = len(all_dataframes[0])\n",
        "\n",
        "for df in all_dataframes:\n",
        "  if len(df) < min_len:\n",
        "    min_len = len(df)\n",
        "\n",
        "df_10s[\"age\"] = 0\n",
        "df_20s[\"age\"] = 1\n",
        "df_30s[\"age\"] = 2\n",
        "\n",
        "all_dataframes = [df_10s.sample(min_len), df_20s.sample(min_len), df_30s.sample(min_len)]\n",
        "\n",
        "df = pd.concat(all_dataframes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoOXbeKhvSRs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKS405cDuOD9"
      },
      "source": [
        "## 3. Create n_grams from the text (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-cEfiq5StIp"
      },
      "source": [
        "# lemmatizing function\r\n",
        "def lemmatize(text):\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\r\n",
        "    return text\r\n",
        "\r\n",
        "# remove stopwords\r\n",
        "def remove_stopwords(text):\r\n",
        "    stopword = nltk.corpus.stopwords.words('english')\r\n",
        "    text = [word for word in text if word not in stopword]\r\n",
        "    return text\r\n",
        "    \r\n",
        "def tokenize(text):\r\n",
        "    wrong_words = [\"urllink\", \"nbsp\"]\r\n",
        "    tokens = [token for token in text.split(\" \") if token != \"\" and token not in wrong_words]\r\n",
        "    return tokens\r\n",
        "\r\n",
        "def lowered(s):\r\n",
        "    return s.lower()\r\n",
        "\r\n",
        "def remove_nonalph(s):\r\n",
        "      s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\r\n",
        "      return s\r\n",
        "\r\n",
        "def generate_ngrams(s, n):\r\n",
        "    # Use the zip function to help us generate n-grams\r\n",
        "    # Concatentate the tokens into ngrams and return\r\n",
        "    ngrams = zip(*[s[i:] for i in range(n)])\r\n",
        "    return [\" \".join(ngram) for ngram in ngrams]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2_X-VvcSwY7"
      },
      "source": [
        "df[\"text\"] = df[\"text\"].apply(lowered).apply(remove_nonalph).apply(tokenize)\r\n",
        "\r\n",
        "\r\n",
        "# ngram = 2\r\n",
        "# for i, text in enumerate(df['text']):\r\n",
        "#   if type(df['text'].iloc[i]) != list:\r\n",
        "#     lower = lowered(text)\r\n",
        "#     non_alph = remove_nonalph(lower)\r\n",
        "#     tokens = tokenize(non_alph)\r\n",
        "#     # no_stopwords = remove_stopwords(tokens)\r\n",
        "#     # lemma = lemmatize(no_stopwords)\r\n",
        "#     # ngrams = generate_ngrams(lemma, ngram)\r\n",
        "#     df['text'].iloc[i] = tokens\r\n",
        "print(df['text'].iloc[0])\r\n",
        "print(df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jih0FPMqymBD"
      },
      "source": [
        "df.index = range(len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuT6rI7YyqrY"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzUULzi2Z_Dd"
      },
      "source": [
        "# AMOUNT_OF_CATEGORIES = 5\r\n",
        "# df = df.assign(age_group=pd.qcut(df['age'], AMOUNT_OF_CATEGORIES, labels=[i for i in range(AMOUNT_OF_CATEGORIES)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IHkERSxa5Vf"
      },
      "source": [
        "# hist_trace = go.Histogram(x=df['age_group'])\r\n",
        "# go.Figure(hist_trace).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vlT5xX7uSAb"
      },
      "source": [
        "## 4. To GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEf8KX5JPkfK"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFBw7zLxuCee"
      },
      "source": [
        "## 5. Initialising Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsg6a_j5mNg7"
      },
      "source": [
        "class TextSentiment(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\r\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\r\n",
        "        self.init_weights()\r\n",
        "        \r\n",
        "    def init_weights(self):\r\n",
        "        initrange = 0.5\r\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\r\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\r\n",
        "        self.fc.bias.data.zero_()\r\n",
        "\r\n",
        "    def forward(self, text, offsets):\r\n",
        "        embedded = self.embedding(text, offsets)\r\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9efH65YV2wi"
      },
      "source": [
        "## 6. Classes preprocessing\r\n",
        "Checking the amount of classes and mapping them each to a different unique number "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCFkJq3MtqvE"
      },
      "source": [
        "class_choice = 'age_group'\r\n",
        "classdict = defaultdict(int)\r\n",
        "\r\n",
        "for row in df[class_choice]:\r\n",
        "  classdict[row] += 1\r\n",
        "print(sorted(classdict))\r\n",
        "\r\n",
        "### PROBLEM: ALS IK DIT WISSEL NAAR NORMAL SORT DAN IS DE UITKOMST INEENS ANDERS???\r\n",
        "classdict = dict(sorted(classdict.items(), key=operator.itemgetter(1), reverse=True))\r\n",
        "#classdict = dict(sorted(classdict.items()))\r\n",
        "print('classdict:')\r\n",
        "print(classdict)\r\n",
        "\r\n",
        "fig = go.Figure([go.Bar(x=('10s', '20s', '30s'), y=[classdict['10s'], classdict['20s'], classdict['30s']])])\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJplh_aSWGbT"
      },
      "source": [
        "# class to number mapping\r\n",
        "classlist = list(classdict.keys())\r\n",
        "classmap = dict([(y,x) for x,y in enumerate(classlist)])\r\n",
        "print('classmap:')\r\n",
        "print(classmap)\r\n",
        "print('amount of classes')\r\n",
        "print(len(classdict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoTwpYirtto3"
      },
      "source": [
        "## 7. Vocabulary dictionary\r\n",
        "Making a dict of all the words in the dataset and mapping each unique word to a unique number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VP2z-L4nOxd"
      },
      "source": [
        "vocabdict = defaultdict(int)   \r\n",
        "\r\n",
        "for row in df['text']:\r\n",
        "  for n_gram in row:\r\n",
        "    n_gram = n_gram.lower()\r\n",
        "    vocabdict[n_gram] += 1\r\n",
        "\r\n",
        "vocabdict = dict(sorted(vocabdict.items(), key=operator.itemgetter(1), reverse=True))\r\n",
        "print(vocabdict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T1uELynoUzs"
      },
      "source": [
        "# WORD TO NUMBER MAPPING\r\n",
        "l = list(vocabdict.keys())\r\n",
        "wordmap = dict([(y,x) for x,y in enumerate(l)])\r\n",
        "print(wordmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJJoVUa_N1zQ"
      },
      "source": [
        "## 8. Mapping text column to numbers (tensor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nse4QPHOBBy"
      },
      "source": [
        "df_copy = df.copy()\r\n",
        "\r\n",
        "for i, text in enumerate(df['text']):\r\n",
        "  newtext = []\r\n",
        "  for word in text:\r\n",
        "    word = word.lower()\r\n",
        "    newtext.append(int(wordmap[word]))\r\n",
        "  df['text'][i] = torch.tensor(newtext).to(torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX0r9-j4Rl37"
      },
      "source": [
        "train_dataset = []\r\n",
        "\r\n",
        "for index, row in df.iterrows():\r\n",
        "  clasn = classmap[row['age_group']]\r\n",
        "  train_dataset.append(tuple((clasn, row['text'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRwRMsSPbYjA"
      },
      "source": [
        "As you can see below the train dataset is now a list with tuples (age, text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZAHGOKnTAQo"
      },
      "source": [
        "print(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcONa66LvDFj"
      },
      "source": [
        "## 9. Run neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-70LB7smiq4"
      },
      "source": [
        "vocab = len(vocabdict)\r\n",
        "embed_dim = 32\r\n",
        "n_classes = len(classdict)\r\n",
        "BATCH_SIZE = 16\r\n",
        "model = TextSentiment(vocab, embed_dim, n_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb4Nn74JvB2R"
      },
      "source": [
        "def generate_batch(batch):\r\n",
        "    label = torch.tensor([entry[0] for entry in batch])\r\n",
        "    text = [entry[1] for entry in batch]\r\n",
        "    offsets = [0] + [len(entry) for entry in text]\r\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\r\n",
        "    # of elements in the dimension dim.\r\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\r\n",
        "\r\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\r\n",
        "    text = torch.cat(text)\r\n",
        "    return text, offsets, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F002_I0D0GPs"
      },
      "source": [
        "### 9.1 Confusion *matrix*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4QgicH-0E4O"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# confusion matrix plot function\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    import itertools\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/Language, speech and dialogue processing/PAN_nn_confusion_matrix.png')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijrN2ezY0s4G"
      },
      "source": [
        "def train_func(sub_train_):\r\n",
        "\r\n",
        "    # Train the model\r\n",
        "    train_loss = 0\r\n",
        "    train_acc = 0\r\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\r\n",
        "                      collate_fn=generate_batch)\r\n",
        "    for i, (text, offsets, cls) in enumerate(data):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\r\n",
        "        output = model(text, offsets)\r\n",
        "        loss = criterion(output, cls)\r\n",
        "        train_loss += loss.item()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\r\n",
        "\r\n",
        "    # Adjust the learning rate\r\n",
        "    scheduler.step()\r\n",
        "\r\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\r\n",
        "\r\n",
        "def test(data_, last_epoch=False):\r\n",
        "    loss = 0\r\n",
        "    acc = 0\r\n",
        "    predictions = []\r\n",
        "    labels = []\r\n",
        "    inv_map = {v: k for k, v in classmap.items()}\r\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\r\n",
        "    for text, offsets, cls in data:\r\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\r\n",
        "        with torch.no_grad():\r\n",
        "            output = model(text, offsets)\r\n",
        "            loss = criterion(output, cls)\r\n",
        "            loss += loss.item()\r\n",
        "            acc += (output.argmax(1) == cls).sum().item()\r\n",
        "        pred = output.argmax(1).to('cpu')\r\n",
        "        label = cls.to('cpu')\r\n",
        "        predictions.extend(pred)\r\n",
        "        labels.extend(label)\r\n",
        "        \r\n",
        "    if last_epoch == False:\r\n",
        "      return loss / len(data_), acc / len(data_)\r\n",
        "    else:\r\n",
        "      return predictions, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-E6whhP0tjq"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "min_valid_loss = float('inf')\r\n",
        "\r\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\r\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\r\n",
        "\r\n",
        "train_len = int(len(train_dataset) * 0.95)\r\n",
        "sub_train_, sub_valid_ = \\\r\n",
        "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "\r\n",
        "  train_loss, train_acc = train_func(sub_train_)\r\n",
        "  valid_loss, valid_acc = test(sub_valid_)\r\n",
        "\r\n",
        "  print('Epoch: %d' %(epoch + 1))\r\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\r\n",
        "  print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\r\n",
        "  if epoch == (N_EPOCHS-1):\r\n",
        "    predictions, labels = test(sub_valid_, True)\r\n",
        "    conf_matrix = confusion_matrix(np.array(labels), np.array(predictions), labels=[0, 1, 2])\r\n",
        "    np.set_printoptions(precision=2)\r\n",
        "    # Plot non-normalized confusion matrix\r\n",
        "    #plt.figure()\r\n",
        "    plot_confusion_matrix(conf_matrix, classes=['10s', '20s', '30s'], title='Confusion matrix, without normalization')\r\n",
        "total_time = int(time.time() - start_time)\r\n",
        "print('Total time elapsed: %d seconds.' %(total_time))\r\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Language, speech and dialogue processing/pan_state.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MQHu4M-vmrq"
      },
      "source": [
        "plot_confusion_matrix(conf_matrix, classes=['10s', '20s', '30s'], normalize=True, title='PAN NN Confusion matrix, with normalization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShkP2G-KTgZS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}